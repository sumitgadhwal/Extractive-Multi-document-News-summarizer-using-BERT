# Extractive-Multi-document-News-summarizer-using-BERT
@Author : Sumit Kumar

**Context**
Multi-document text summarisation models can efficiently find informative sentences from a set of documents provided of same domain.Extractive text summarization aims at extracting sentences from original set of documents according to some determined features, and these features decide which sentences are more important and precedence over other sentences. By this process we determine important sentences and include them in summary.

**BERT (Bidirectional Encoder Representations from Transformers)**
BERT is a pre-trained model developed and trained by Google. As BERT is already pretrained or a large dataset, it saves us a lots of efforts of training the neural network. BERT has further increased performance in NLP(Natural Language Processing) tasks which makes it more desirable to use in comparison to other models.
BERT learns words from a large text corpus and design a vector form of representation of words given and captures its semantic relationship with other words present in its text corpus. BERT model takes the sentences in input and returns word embeddings in the output. BERT has better understanding of text as compared to other model because it reads the whole text at once instead of reading it from forward to backward or in one direction.
BERT gives us output vectors , these vectors are represented in the form of tokens. Now BERT alone does not gives us summarization of text, It only gives us word  embeddings of sentences. We further need to fine-tune BERT which means apply various algorithms on BERT to generate summary from our articles.Our proposed model is proposed in two steps, First step is where we process our text and generate word embeddings with help of BERT. In second step many models are used on word embeddings generated by BERT model to give us good summarization results. We use various models for fine tuning BERT which are given below:

**1. K-means Algorithm**
In K-means algorithm, we divide all the data into a predefined number of clusters, each of these cluster contain some part of data. In the same cluster it tries to put the datapoints which share more similarity. One important factor in this algorithm is deciding the value of number of clusters. As having too large value or too small value of cluster results in poor results so we will have to decide optimum value of number of clusters using some technique.

**2. Centroid-based through Compositionality of BERT Word Embedding**
In ordinary centroid-based method, tf.idf score is calculated of the vocabulary words and those which exceed a given threshold value, are taken as centroid words. The sentences which are comprising of most of the centroid words will be taken as part of summary.

**3.MMR (Maximal Marginal Relevance)**
MMR is a model that is used to ensure that there is lesser redundancy in our final results. As we work on multi document summarization of news articles, many articles will have same information and that information may be selected into results so we need to remove redundancy before giving final results.

**4. Sentence Ordering**
We have used a simple greedy approach to order sentences in multi-document summarisation. The assumption taken here is that a good sentence ordering in summary implies that there should be good level of similarity between all adjacent sentences as repetition of word is one of the sign of text coherence.


We measured our results with the help of ROUGE matrix on DUC2002 dataset. The ROUGE value of resultant summary of every stage is compared with the reference summary provided in the dataset. It can be easily seen from below figure that the ROUGE value keep on improving as we add other stages to the model.

**Code**
let's start with run.py file, it will take mentioned files in input directory and give us output summary. We need to define number of sentences for final summary. Now running  'python3 run.py' will do the rest of the job.
